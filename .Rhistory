method="circle",
colors = c("tomato2", "white", "springgreen3"),
title="Correlogram of PKY",
ggtheme=theme_bw)
summary(correlations)
correlations
hist(correlations)
correlations.selected <- findCorrelation(correlations, cutoff = .2, exact = TRUE)
grade.columns <- 15:17
corr <- cor(alc2[c(correlations.selected)])
corr
ggcorrplot(corr, hc.order = TRUE,
type = "lower",
lab = TRUE,
lab_size = 3,
method="circle",
colors = c("tomato2", "white", "springgreen3"),
title="Correlogram of PKY",
ggtheme=theme_bw)
correlations <- abs(cor(alc2, use="pairwise.complete.obs"))
#correlation of all parameters with absolute numbers
correlations.selected <- findCorrelation(correlations, cutoff = .1, exact = TRUE)
#colums with to select by cutoff 0.7
#Making sure that grades are part of the correlation table
grade.columns <- 15:17
correlations.selected <- union(correlations.selected,grade.columns )
# calculating the variables
corr <- cor(alc2[c(correlations.selected)])
#correlations of narrow selection and grades variables G1, G2, G3
# Ploting correaltions
ggcorrplot(corr, hc.order = TRUE,
type = "lower",
lab = TRUE,
lab_size = 3,
method="circle",
colors = c("tomato2", "white", "springgreen3"),
title="Correlogram of PKY",
ggtheme=theme_bw)
?sbf
test <- sbf(alc3, sbfControl( repeats = 5)
)
test <- sbf(alc, sbfControl( repeats = 5))
data(BloodBrain)
names(bbbDescr)
bbbDescr$logBBB
RFwithGAM <- sbf(bbbDescr, logBBB,
sbfControl = sbfControl(functions = rfSBF,
verbose = FALSE,
method = "cv"))
library(gam)
install.packages("gam")
RFwithGAM <- sbf(bbbDescr, logBBB,
sbfControl = sbfControl(functions = rfSBF,
verbose = FALSE,
method = "cv"))
summary(RFwithGAM)
install.packages("randomForest")
library(gam)
library(randomForest)
RFwithGAM <- sbf(bbbDescr, logBBB,
sbfControl = sbfControl(functions = rfSBF,
verbose = FALSE,
method = "cv"))
glimpse(RFwithGAM)
summary(RFwithGAM)
plot(RFwithGAM)
RFwithGAM$variables
confusionMatrix(RFwithGAM)
Test <- sbf(alc, logBBB,
sbfControl = sbfControl(functions = rfSBF,
verbose = TRUE,
method = "cv"))
confusionMatrix(Test)
Test <- sbf(alc, logBBB,
sbfControl = sbfControl(functions = rfSBF,
verbose = TRUE,
method = "cv", saveDetails = TRUE))
Test <- sbf(alc, logBBB,
sbfControl = sbfControl(functions = rfSBF,
verbose = TRUE,
method = "cv", saveDetails = TRUE))
confusionMatrix(test)
confusionMatrix(Test)
summary(Test)
summary(Test$pred)
summary(Test$pred$predictions)
Test$pred$predictions
warnings()
str(alc)
alc[-1,-36]
alc[c(-1,-36)]
5+1
alc(1)
alc[1]
alc <- read.csv(file = "data/ex3alc.csv")
getwd()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list=ls())
alc <- read.csv(file = "data/ex3alc.csv")
get(wd)
getwd()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #works in Rstudio only
knitr::opts_chunk$set(echo = TRUE)
alc <- read.csv(file = "data/ex3alc.csv")
packages_to_load <- c("dplyr", "caret", "ggcorrplot", "ggplot2", "GGally")
glimpse(alc)
summary(alc)
# The data set is combined from two different files from the machine learning repository
#alc is the full dataset, we take numeric values for correlation analysis
alc2 <- select_if(alc, is.numeric)
correlations <- abs(cor(alc2, use="pairwise.complete.obs"))
#correlation of all parameters with absolute numbers
correlations.selected <- findCorrelation(correlations, cutoff = .1, exact = TRUE)
#colums with to select by cutoff 0.7
#Making sure that grades are part of the correlation table
grade.columns <- 15:17
correlations.selected <- union(correlations.selected,grade.columns )
# calculating the variables
corr <- cor(alc2[c(correlations.selected)])
#correlations of narrow selection and grades variables G1, G2, G3
# Ploting correaltions
ggcorrplot(corr, hc.order = TRUE,
type = "lower",
lab = TRUE,
lab_size = 3,
method="circle",
colors = c("tomato2", "white", "springgreen3"),
title="Correlogram of PKY",
ggtheme=theme_bw)
# Now we see the highest correlation to G1:G3 is with age, Medu, alc_use and Walc. Since alc_use and Walc have high correlation, only better one is selected for the analysis. G3 is the final grade, so we first try to only predict that and drop the G1 and G2, even thought it would be likely that they would be good predictors of G3.
# So we want to predict the Final grade of the students, with their alcohol use, age and their mothers education level.
# I hypotise that alcohol use would reduce the performance and mothers educational level would increase their performance.
my_fn <- function(data, mapping, ...){
p <- ggplot(data = data, mapping = mapping) +
geom_point() +
geom_smooth(method=loess, fill="red", color="red", ...) +
geom_smooth(method=lm, fill="blue", color="blue", ...)
p
}
g = ggpairs(alc2,columns = c(correlations.selected), lower = list(continuous = my_fn))
g
# Lets add some factor variables to the data
names.selected <- colnames(alc2[correlations.selected])
alc3 <- subset(alc, select=c(names.selected, "sex", "school", "guardian", "activities"))
p <-ggpairs(alc, mapping = aes(col = sex, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
# The categorial variables are quite similar and does not give much indication other than, that the very alcohol use is higher with males and almost no alcohol use is more common with females.
m <- glm(G3 ~ alc_use + Walc + age + Medu, family = "gaussian", data = alc3)
m2 <- glm(G3 ~ alc_use + age + Medu, family = "gaussian", data = alc3)
deviance(m2)/m2$null.deviance
AIC(m,m2)
par(mfrow=c(2,2))
plot(m2)
# Akaike information criteria gives similar result, so as hypotised the Walc is dropped as they have covariance, that way we achive the most parsimonious model.
# Model explains (R^2) 91 % of the variation without the help of G1 and G2.
# !!! Oh, I only realized at this point that the target variable was supposed to be high/low alcohol consumption, now I realize why logistic regression in this case... !!!
# I suppose I need to do another selection for parameters
packages_to_load <- c("dplyr", "caret", "ggcorrplot", "ggplot2", "GGally")
lapply(packages_to_load, require, character.only = TRUE)
glimpse(alc)
summary(alc)
# The data set is combined from two different files from the machine learning repository
#alc is the full dataset, we take numeric values for correlation analysis
alc2 <- select_if(alc, is.numeric)
correlations <- abs(cor(alc2, use="pairwise.complete.obs"))
#correlation of all parameters with absolute numbers
correlations.selected <- findCorrelation(correlations, cutoff = .1, exact = TRUE)
#colums with to select by cutoff 0.7
#Making sure that grades are part of the correlation table
grade.columns <- 15:17
correlations.selected <- union(correlations.selected,grade.columns )
# calculating the variables
corr <- cor(alc2[c(correlations.selected)])
#correlations of narrow selection and grades variables G1, G2, G3
# Ploting correaltions
ggcorrplot(corr, hc.order = TRUE,
type = "lower",
lab = TRUE,
lab_size = 3,
method="circle",
colors = c("tomato2", "white", "springgreen3"),
title="Correlogram of PKY",
ggtheme=theme_bw)
# Now we see the highest correlation to G1:G3 is with age, Medu, alc_use and Walc. Since alc_use and Walc have high correlation, only better one is selected for the analysis. G3 is the final grade, so we first try to only predict that and drop the G1 and G2, even thought it would be likely that they would be good predictors of G3.
# So we want to predict the Final grade of the students, with their alcohol use, age and their mothers education level.
# I hypotise that alcohol use would reduce the performance and mothers educational level would increase their performance.
my_fn <- function(data, mapping, ...){
p <- ggplot(data = data, mapping = mapping) +
geom_point() +
geom_smooth(method=loess, fill="red", color="red", ...) +
geom_smooth(method=lm, fill="blue", color="blue", ...)
p
}
g = ggpairs(alc2,columns = c(correlations.selected), lower = list(continuous = my_fn))
g
# Lets add some factor variables to the data
names.selected <- colnames(alc2[correlations.selected])
alc3 <- subset(alc, select=c(names.selected, "sex", "school", "guardian", "activities"))
p <-ggpairs(alc, mapping = aes(col = sex, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
m <- glm(G3 ~ alc_use + Walc + age + Medu, family = "gaussian", data = alc3)
m2 <- glm(G3 ~ alc_use + age + Medu, family = "gaussian", data = alc3)
deviance(m2)/m2$null.deviance
AIC(m,m2)
par(mfrow=c(2,2))
plot(m2)
data(BloodBrain)
filterCtrl <- sbfControl(functions = rfSBF, method = "repeatedcv", repeats = 5)
RFwithGAM <- sbf(bbbDescr, logBBB,
sbfControl = filterCtrl)
RFwithGAM
test <- sbf(alc, alc$G3, sbfControl(filterCtrl)
)
logBBB
alc$G3
test2 <- preProcess(alc[, -1],
method = c("center", "scale", "YeoJohnson"))
test2
transformed <- predict(test2, newdata = alc[, -1])
test <- sbf(transformed, sbfControl(filterCtrl)
data(BloodBrain)
test <- sbf(transformed, sbfControl(filterCtrl))
test <- sbf(transformed, transformed$G3, sbfControl(filterCtrl))
transformed$G3
transformed <- predict(test2, newdata = alc[, -30:36])
transformed <- predict(test2, newdata = alc[, -c(30:36)])
transformed <- predict(test2, newdata = alc[,2:30])
test2 <- preProcess(alc[2:30],
method = c("center", "scale", "YeoJohnson"))
transformed <- predict(test2, newdata = alc[2:30])
test <- sbf(transformed, alc$G3, sbfControl(filterCtrl))
fit <- sbf(
form = alc$G3 ~ .,
data = transformed, method = "glmnet",
tuneGrid=expand.grid(.alpha = .01, .lambda = .1),
preProc = c("center", "scale"),
trControl = trainControl(method = "none"),
sbfControl = sbfControl(functions = caretSBF, method = 'cv', number = 10)
)
fit
names(alc[2:30])
names(alc[2:31])
names(alc[2:33])
fit <- sbf(
form = alc$G3 ~ .,
data = alc[2:31], method = "glmnet",
tuneGrid=expand.grid(.alpha = .01, .lambda = .1),
preProc = c("center", "scale"),
trControl = trainControl(method = "none"),
sbfControl = sbfControl(functions = caretSBF, method = 'cv', number = 10)
)
fit
fit <- sbf(
form = alc$alc_use ~ .,
data = alc[2:34], method = "glmnet",
tuneGrid=expand.grid(.alpha = .01, .lambda = .1),
preProc = c("center", "scale"),
trControl = trainControl(method = "none"),
sbfControl = sbfControl(functions = caretSBF, method = 'cv', number = 10)
)
fit
str(fit)
confusionMatrix(fit)
confusionMatrix.train(fit)
fit
bm <- glm(high_use ~ failures + absences+ Dalc + freetime, data = alc, family = "binomial")
bm
summary(bm)
step(bm)
bm <- glm(high_use ~ failures + absences + freetime, data = alc, family = "binomial")
summary(bm)
plot(bm)
fit
str(fit)
str(bm)
bm$R
bm
step(bm)
fit
bm <- glm(high_use ~ failures + absences + freetime + age, data = alc, family = "binomial")
summary(bm)
step(bm)
bm <- glm(high_use ~ failures + absences + freetime + age, data = alc, family = "binomial")
bm2 <- glm(high_use ~ failures + absences + freetime, data = alc, family = "binomial")
aic(bm,bm2)
AIC(bm,bm2)
?step
step(bm, direction = "forward")
step(bm, direction = "both")
bm3 <- glm(high_use ~ absences, data = alc, family = "binomial")
step(bm3, direction = "both")
names(alc)
names(alc[28])
names(alc[28:29])
fit <- sbf(
form = alc$alc_use ~ .,
data = alc[c(2:7,29:34)], method = "glmnet", # Dalc and Walc are dropped as they are the parameters high_use is based on
tuneGrid=expand.grid(.alpha = .01, .lambda = .1),
preProc = c("center", "scale"),
trControl = trainControl(method = "none"),
sbfControl = sbfControl(functions = caretSBF, method = 'cv', number = 10)
)
fit
bm3 <- glm(high_use ~ absences + age + G1+ G2+ G3, data = alc, family = "binomial")
summary(bm3)
step(bm3, direction = "both")
bm4 <- glm(high_use ~ absences + age + G1, data = alc, family = "binomial")
summary(bm4)
AIC(bm,bm2,bm3,bm4)
names(alc)
names(alc[31])
fit <- sbf(
form = alc$alc_use ~ .,
data = alc[c(2:27,29:31)], method = "glmnet", # Dalc and Walc are dropped as they are the parameters high_use is based on, D1:D3, dropped as well
tuneGrid=expand.grid(.alpha = .01, .lambda = .1),
preProc = c("center", "scale"),
trControl = trainControl(method = "none"),
sbfControl = sbfControl(functions = caretSBF, method = 'cv', number = 10)
)
fit
bm5 <- glm(high_use ~ failures + absences + freetime + age + goout, data = alc, family = "binomial")
summary(bm5)
AIC(bm,bm2,bm3,bm4,bm5)
step(bm5, direction = "both")
bm6 <- glm(high_use ~ failures + absences + goout, data = alc, family = "binomial")
summary(bm6)
AIC(bm,bm2,bm3,bm4,bm5,bm6)
plot(bm6)
library(MASS)
params = fitdistr(alc$high_use, "Binomial")
hist(alg$high_use)
hist(alc$high_use)
summary(alc$high_use)
length(alc$high_use)
fitBinom=fitdist(data=alc$high_use, dist="binom", fix.arg=list(size=382), start=list(prob=0.3))
fitBinom=fitdistr(data=alc$high_use, dist="binom", fix.arg=list(size=382), start=list(prob=0.3))
fitBinom=fitdistr(data=as.numeric(alc$high_use), dist="binom", fix.arg=list(size=382), start=list(prob=0.3))
as.numeric(alc$high_use
)
rbinom(n=40,size=8,prob=.25)
rbinom(n=40,size=2,prob=.25)
fitBinom=fitdistr(data=as.numeric(alc$high_use), dist="binom", fix.arg=list(size=2), start=list(prob=0.3))
alc_num <- as.numeric(alc$high_use)
fitBinom=fitdistr(data=alc_num, dist="binom", fix.arg=list(size=2), start=list(prob=0.3))
library(fitdistrplus)
fitBinom=fitdist(data=alc_num, dist="binom", fix.arg=list(size=2), start=list(prob=0.3))
plot(fitBinom)
?fitdistrplus
fitBinom
str(fitBinom)
plot(qnbinom(ppoints(alc_num), size=2, mu=fitBinom$estimate), sort(alc_num))
fitBinom$estimate
plot(qnbinom(ppoints(alc_num), size=2, mu=0.1492146), sort(alc_num))
plot(pnbinom(sort(alc_num), size=2, mu=fitBinom$estimate), ppoints(alc_num))
abline(0,1)
summary(bm6)
par(mfrow=c(2,2))
plot(bm6)
?predict
test <- predict(bm3)
test
hist(test)
test <- predict(bm6)
hist(test)
test
OR <- coef(bm6) %>% exp
CI <- exp(confint(bm6))
cbind(OR, CI)
summary(bm6)
View(alc)
fit <- sbf(
form = alc$alc_use ~ .,
data = alc[c(2:27,29:31)], method = "glmnet", # Dalc and Walc are dropped as they are the parameters high_use is based on, D1:D3, dropped as well
tuneGrid=expand.grid(.alpha = .01, .lambda = .1),
preProc = c(), # preProc = c("center", "scale"),
trControl = trainControl(method = "none"),
sbfControl = sbfControl(functions = caretSBF, method = 'cv', number = 10)
)
fit
pred.bm6 <- predict(bm6, type = "response")
hist(pred.bm6)
?sbf
alc <- mutate(alc, probability = pred.bm6)
alc <- mutate(alc, prediction = pred.bm6 > 0.5)
select(alc, failures, absences, sex, high_use, probability, prediction) %>% tail(10)
select(alc, failures, absences, goout, high_use, probability, prediction) %>% tail(10)
select(alc, failures, absences, goout) %>% tail(10)
?select
MASS::select(alc, failures, absences, goout) %>% tail(10)
MASS::select(alc, failures, absences, goout)
MASS::select(alc, failures)
names(alc)
alc[35:38] %>% tail(10)
sum(alc[36]=alc[38])
sum(alc[36]==alc[38])
sum(alc[36]==alc[38])/lenght(alc[38])
sum(alc[36]==alc[38])/length(alc[38])
291/length(alc[38])
length(alc[38])
alc[38]
length(alc)
dim(alc)
sum(alc[36]==alc[38])/382
sum(alc[36]==alc[38])/382*100
table(high_use = alc$high_use, prediction = alc$prediction)
1-sum(alc[36]==alc[38])/382
g <- ggplot(alc, aes(x = high_use, y = probability, color = sex))
g + geom_point()
g <- ggplot(alc, aes(x = c(high_use, prediction), y = probability, color = sex))
g + geom_point()
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
loss_func(class = alc$high_use, prob = alc$probability)
sessionInfo
sessionInfo()
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
alc <- read.csv(file = "data/ex3alc.csv")
packages_to_load <- c("dplyr", "caret", "ggcorrplot", "ggplot2", "GGally", "bindrcpp", "lattice")
lapply(packages_to_load, require, character.only = TRUE)
glimpse(alc)
summary(alc)
alc2 <- select_if(alc, is.numeric)
correlations <- abs(cor(alc2, use="pairwise.complete.obs"))
correlations.selected <- findCorrelation(correlations, cutoff = .1, exact = TRUE)
grade.columns <- 15:17
correlations.selected <- union(correlations.selected,grade.columns )
corr <- cor(alc2[c(correlations.selected)])
ggcorrplot(corr, hc.order = TRUE,
type = "lower",
lab = TRUE,
lab_size = 3,
method="circle",
colors = c("tomato2", "white", "springgreen3"),
title="Correlogram of PKY",
ggtheme=theme_bw)
my_fn <- function(data, mapping, ...){
p <- ggplot(data = data, mapping = mapping) +
geom_point() +
geom_smooth(method=loess, fill="red", color="red", ...) +
geom_smooth(method=lm, fill="blue", color="blue", ...)
p
}
g = ggpairs(alc2,columns = c(correlations.selected), lower = list(continuous = my_fn))
g
# Lets add some factor variables to the data
names.selected <- colnames(alc2[correlations.selected])
alc3 <- subset(alc, select=c(names.selected, "sex", "school", "guardian", "activities"))
p <-ggpairs(alc2, mapping = aes(col = sex, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
p <-ggpairs(alc3, mapping = aes(col = sex, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
p <-ggpairs(sort(alc3), mapping = aes(col = sex, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
m <- glm(G3 ~ alc_use + Walc + age + Medu, family = "gaussian", data = alc3)
m2 <- glm(G3 ~ alc_use + age + Medu, family = "gaussian", data = alc3)
deviance(m2)/m2$null.deviance
AIC(m,m2)
par(mfrow=c(2,2))
plot(m2)
alc2 <- select_if(alc, is.numeric)
correlations <- abs(cor(alc2, use="pairwise.complete.obs"))
#correlation of all parameters with absolute numbers
correlations.selected <- findCorrelation(correlations, cutoff = .3, exact = TRUE)
#colums with to select by cutoff 0.7
#Making sure that grades are part of the correlation table
grade.columns <- 15:17
correlations.selected <- union(correlations.selected,grade.columns )
# calculating the variables
corr <- cor(alc2[c(correlations.selected)])
#correlations of narrow selection and grades variables G1, G2, G3
# Ploting correaltions
ggcorrplot(corr, hc.order = TRUE,
type = "lower",
lab = TRUE,
lab_size = 3,
method="circle",
colors = c("tomato2", "white", "springgreen3"),
title="Correlogram of PKY",
ggtheme=theme_bw)
install.packages("boot")
cv <- cv.glm(data = alc, cost = loss_func, glmfit = bm6, K = nrow(alc))
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = bm6, K = nrow(alc))
bm6 <- glm(high_use ~ failures + absences + goout, data = alc, family = "binomial")
cv <- cv.glm(data = alc, cost = loss_func, glmfit = bm6, K = nrow(alc))
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = bm6, K = nrow(alc))
cv$delta[1]
cv <- cv.glm(data = alc, cost = loss_func, glmfit = bm6, K = 10)
cv$delta[1]
date()
packages_to_load <- c("dplyr", "MASS", "tidyverse", "corrplot")
lapply(packages_to_load, require, character.only = TRUE)
install.packages("corrplt")
install.packages("corrplot")
install.packages("tidyverse")
rm(list=ls())
install.packages("tidyverse")
.libPaths()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #works in Rstudio only
packages_to_load <- c("dplyr", "MASS", "tidyverse", "corrplot")
lapply(packages_to_load, require, character.only = TRUE)
if(!require(installr)) {
install.packages("installr"); require(installr)}
updateR()
