---
title: "Chapter 5. Clustering and classification"
author: "Tuure Parviainen"
date: "November 22, 2017"
output: html_document
---
## Chapter 5. Dimensionality reduction techniques

I completed all the DataCamp exercises prior to proceeding to this exercise. Today we are foraying to the mystical world of GNI and then it's time for tea.

I found this tip for making the Rmarkdown neater. This looks bit similar solution than I used. I stored the css in seperate file that the Knitr can load from "styles.css". Look at the index file if interested.


M<style type="text/css">
Mbody{ /* Normal  */
Mfont-size: 20px;
M}
M/* Thanks for Hans Hellen for this tip! */
M</style>

First the GNI.

### GNI Data set information:

Variables     | Description
----------    | --------------------------------------
Edu2.FM       | Secondary education Female/male index
Labo.F        | Female labour participation rate
Life.Exp      | Life expentancy at birth
Edu.Exp       | Educational expentancy
GNI           | Gender Development index
Mat.Mor       | Mortality of mothers
Ado.Birth     | Adolecent birth rate
Parli.F       | Parlamentary represtantion of females



```{r, message=FALSE, warning=FALSE}

require("easypackages") # for loading and installing many packages at one time
packages_to_load <- c("broom", "dplyr", "tidyverse", "corrplot", "ggplot2", "GGally", "devtools", "ggthemes", "stringr", "FactoMineR", "factoextra")
packages(packages_to_load, prompt = TRUE) # lock'n'load install/load


```

```{r, message=FALSE, warning=FALSE}
human <- read.table("data/humaf.csv", sep  =",", header = T, row.names = 1)
glimpse(human)

```



```{r, message=FALSE, warning=FALSE}
ggpairs(
  human, 1:8,
  lower = list(combo = wrap("facethist", binwidth = 0.5
)))


ggduo(
  human, 5, c(1:4,6:8),
  types = list(continuous = "smooth_loess", mapping = aes(color = GNI)),
  title = "GNI constituents loess smooth",
  xlab = "",
  ylab = ""
)

```

1. The data seems non-normal, and all variables seems to have strong correlation to GNI except Lab.FM.
2. Many of the variables seems to have non-linear or perhaps logaritmic relationships (blue lines in the picture).

```{r, message=FALSE, warning=FALSE}

human_std <- scale(human) %>% as.data.frame()
ggpairs(
  human_std, 1:8,
  lower = list(combo = wrap("facethist", binwidth = 0.5
  )))

ggduo(
  human_std, 5, c(1:4,6:8),
  types = list(continuous = "smooth_loess", mapping = aes(color = GNI)),
  title = "GNI constituents loess smooth",
  xlab = "",
  ylab = ""
)


```

The scaled dataset looks very similar, however, some extreme values are now drawn more closer.
```{r, message=FALSE, warning=FALSE}
# Running the principal component analysis'
pca_human       <- prcomp(human)
pca_human_std   <- prcomp(human_std)

# comparison summaries
s_pca_human     <- summary(pca_human)
s_pca_human_std <- summary(pca_human_std)
s_pca_human
s_pca_human_std

# Generating labels
pca_pr      <- round(100*s_pca_human$importance[2, ], digits = 1)
pca_pr_std <- round(100*s_pca_human_std$importance[2, ], digits = 1)
pca_pr_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
pca_pr_lab_std <- paste0(names(pca_pr_std), " (", pca_pr_std, "%)")

#Plotting 2

biplot(
  pca_human, cex = c(0.8, 1),
  col = c("grey40", "deeppink2"),
  xlab = pca_pr_lab[1],
  ylab = pca_pr_lab[2],
  main= "Principal component analysis of GNI components (unstandardized)"
)
biplot(
  pca_human_std,
  cex = c(0.8, 1),
  col = c("black", "green2"),
  xlab = pca_pr_lab_std[1],
  ylab = pca_pr_lab_std[2],
  main= "Principal component analysis of GNI components (standardized)"
)


```

Non-scaled results are non-usable. All the nearly all deviation are explained by the first dimension as the uneven scale makes the big numbers have huge importance.

### GNI Data standardized PCA results for the 1st and 2nd dimension:

Variables     | Description                             | PC1       | PC2
----------    | ---------                               |------     |------
Edu2.FM       | Secondary education Female/male index   |-0.35664370| 0.03796058
Labo.F        | Female labour participation rate        | 0.05457785| 0.72432726
Life.Exp      | Life expentancy at birth                |-0.44372240|-0.02530473
Edu.Exp       | Educational expentancy                  |-0.42766720| 0.13940571
GNI           | Gender Development index                |-0.35048295| 0.05060876
Mat.Mor       | Mortality of mothers                    | 0.43697098| 0.14508727
Ado.Birth     | Adolecent birth rate                    | 0.41126010| 0.07708468
Parli.F       | Parlamentary represtantion of females   |-0.08438558| 0.65136866

PCA splits the data to eight dimensions where first three explain 79 % of the variance.
First two dimensions explain 69 %. Horizontal dimension is explained by education and life expectancy and Gender development index, and to the right (positive) dimension is explained by mortality of mothers and adolecent brith rate. 2nd dimension is explained by parlamentary representation of females and labor participation ratio.

```{r, message=FALSE, warning=FALSE}

data(tea)

keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
tea_time %>% summary()
#str(tea_time)

# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
 mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)
```


```{r, message=FALSE, warning=FALSE}
# visualize MCA
 plot(mca, invisible=c("ind"), habillage = "quali")

 plot(mca, habillage = "quali")
 
 # Screeplot dimension explaining for variance
 fviz_screeplot(mca, addlabels = TRUE, ylim = c(0, 50))
 
 #Strongest 1 dimension
 fviz_mca_ind(mca,  habillage = "how",
              addEllipses = TRUE, repel = TRUE)
 #Stronggest 2 dimension
 fviz_mca_ind(mca,  habillage = "where",
           addEllipses = TRUE, repel = TRUE)
 fviz_mca_var(mca, repel = TRUE) 

```

1. Here none of the variables have such strong explatory power as in the example before. 
2. First four dimensions explain roughly 50 % of the variance.
3. "How"" and "where" variables split the 1st and the 2nd dimension, where as the 6 dimensions, the separation of the individuals by them is not good. 
4. Basically the individuals in the data set is best separate to groups by where they drink ther tea or do the drink it in premade bags or tea bags.
5. Perhaps the other notable charasteristic was the weather you like green tea or earl grey, since they seemed to differ in the dataset (3rd dimension). 